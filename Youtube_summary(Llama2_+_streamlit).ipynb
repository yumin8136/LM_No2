{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNsX7ve3Y6WsOkusEaz6vnk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yumin8136/LM_No2/blob/main/Youtube_summary(Llama2_%2B_streamlit).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU transformers accelerate einops langchain xformers bitsandbytes youtube-transcript-api streamlit"
      ],
      "metadata": {
        "id": "XyKdp5XAqdhb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oNmmqDv_qJKC",
        "outputId": "1fe7ae35-c028-4cce-8fb9-e75aaf093d6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing youtube_video_search.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile youtube_video_search.py\n",
        "#Llama 2模型初始化, GPU模型设置\n",
        "from torch import cuda,bfloat16\n",
        "import transformers\n",
        "model_id = 'meta-llama/Llama-2-7b-chat-hf'\n",
        "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
        "bub_config = transformers.BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type='nf4',\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=bfloat16\n",
        ")\n",
        "hf_auth = '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'\n",
        "model_config =transformers.AutoConfig.from_pretrained(\n",
        "    model_id,\n",
        "    use_auth_token=hf_auth\n",
        ")\n",
        "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    trust_remote_code=True,\n",
        "    config=model_config,\n",
        "    quantization_config=bub_config,\n",
        "    device_map='auto',\n",
        "    use_auth_token=hf_auth\n",
        ")\n",
        "model.eval()\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
        "    model_id,\n",
        "    use_auth_token = hf_auth\n",
        ")\n",
        "stop_list = ['\\nHuman:','\\n\"\"\"\\n']\n",
        "stop_token_ids = [tokenizer(x)['input_ids'] for x in stop_list]\n",
        "import torch\n",
        "stop_token_ids = [torch.LongTensor(x).to(device) for x in stop_token_ids]\n",
        "from transformers import StoppingCriteria, StoppingCriteriaList\n",
        "class StopOnTokens(StoppingCriteria):\n",
        "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
        "        for stop_ids in stop_token_ids:\n",
        "            if torch.eq(input_ids[0][-len(stop_ids):], stop_ids).all():\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "stopping_criteria = StoppingCriteriaList([StopOnTokens()])\n",
        "generate_text = transformers.pipeline(\n",
        "    model=model, tokenizer=tokenizer,\n",
        "    return_full_text=True,  # langchain expects the full text\n",
        "    task='text-generation',\n",
        "    # we pass model parameters here too\n",
        "    #stopping_criteria=stopping_criteria,  # without this model rambles during chat\n",
        "    temperature=0.0,  # 'randomness' of outputs, 0.0 is the min and 1.0 the max\n",
        "    max_new_tokens=128, #512,  # max number of tokens to generate in the output\n",
        "    repetition_penalty=1.1  # without this output begins repeating\n",
        ")\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "llm = HuggingFacePipeline(pipeline=generate_text)\n",
        "\n",
        "\n",
        "def llm_sum(text):\n",
        "  prompt = f\"\"\"Summarize the full text in a few sentences.\n",
        "  text: ```{text}```\n",
        "  \"\"\"\n",
        "  sum_text = llm(prompt=prompt)\n",
        "  #print(sum_text)\n",
        "  return sum_text\n",
        "#通过url获取视频字母\n",
        "def get_video_subtitle(url):\n",
        "  try:\n",
        "    from langchain.document_loaders import YoutubeLoader\n",
        "    loader = YoutubeLoader.from_youtube_url(url)\n",
        "    subtitle_list = loader.load()\n",
        "    subtitle = subtitle_list[0].page_content\n",
        "  except:\n",
        "    print(\"没有找到视频的字幕\")\n",
        "    return \"None\"\n",
        "  return subtitle\n",
        "\n",
        "#V3.1 将文件分割喂给Llama2 summarize,然后再将分割并summarize的文本合并再次喂给Llama2进行summarize.\n",
        "def sum_subtitles(texts):\n",
        "  from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "  text_splitter = RecursiveCharacterTextSplitter(chunk_size = 5000, chunk_overlap =0)\n",
        "  chunks =text_splitter.split_text(texts)\n",
        "\n",
        "  sums =\"\"\n",
        "  sum_subtitles =[]\n",
        "  for i in range(len(chunks)):\n",
        "    sum_subtitle = llm_sum(chunks[i])\n",
        "    sum_subtitles.append(sum_subtitle)\n",
        "    #print(str(i)+\":\"+sum_subtitle+\"\\n\")\n",
        "    sums = sums + sum_subtitles[i] +\"\\n\"\n",
        "  print(sums)\n",
        "  #sum = llm(sums)\n",
        "  return sums\n",
        "\n",
        "#输入关键词，获取批量相关youtube视频信息,返回视频名,视频url,视频字幕，获取前5条记录\n",
        "def get_video_info_from_youtube_crawler(keyword):\n",
        "    import requests\n",
        "    import re\n",
        "    import json\n",
        "    url = \"https://m.youtube.com/results?search_query=\" + keyword\n",
        "    print(\"通过爬虫获取数据\")\n",
        "    resp = requests.get(url)\n",
        "\n",
        "    if resp.status_code == 200:\n",
        "        result_json = re.findall(r'ytInitialData = (.*);</script>', resp.text)[0]\n",
        "        result_obj = json.loads(result_json)\n",
        "\n",
        "        video_names = []\n",
        "        video_urls = []\n",
        "        video_subtitles = []\n",
        "        summ_subtitles = []\n",
        "\n",
        "        try:\n",
        "            contents = result_obj['contents']['twoColumnSearchResultsRenderer']['primaryContents']['sectionListRenderer']['contents']\n",
        "            count = 0\n",
        "            for content in contents:\n",
        "                if 'itemSectionRenderer' in content and 'contents' in content['itemSectionRenderer']:\n",
        "                    videos = content['itemSectionRenderer']['contents']\n",
        "                    for video in videos:\n",
        "                        if 'videoRenderer' in video:\n",
        "                            video_id = video['videoRenderer']['videoId']\n",
        "                            video_name = video['videoRenderer']['title']['runs'][0]['text']\n",
        "                            video_url = \"https://www.youtube.com/watch?v=\" + video_id\n",
        "                            video_subtitle = get_video_subtitle(video_url)\n",
        "                            video_names.append(video_name)\n",
        "                            video_urls.append(video_url)\n",
        "                            video_subtitles.append(video_subtitle)\n",
        "                            sum_subtitle = sum_subtitles(video_subtitle)\n",
        "                            summ_subtitles.append(sum_subtitle)\n",
        "                            count += 1\n",
        "                            #获取前五条记录\n",
        "                            if count >= 2:\n",
        "                              break\n",
        "\n",
        "        except KeyError:\n",
        "            return [], [] , [] ,[]\n",
        "\n",
        "        return video_names, video_urls ,video_subtitles, summ_subtitles\n",
        "    else:\n",
        "        return [], [], [], []\n",
        "\n",
        "#将视频名，视频url,视频字幕，路径等信息保存到本地\n",
        "def store_video_info_to_files(video_names, video_urls, video_subtitles, folder_path ,sum_subtitles):\n",
        "    import re\n",
        "    for name, url, subtitle ,sum_subtitle in zip(video_names, video_urls ,video_subtitles ,sum_subtitles):\n",
        "        file_name = re.sub(r'[\\/:*?\"<>|]', '_', name) + \".txt\"\n",
        "        #file_name = name + \".txt\"\n",
        "        file_path = folder_path + \"/\" + file_name\n",
        "        #print(file_path)\n",
        "        print(name)\n",
        "        print(url)\n",
        "        with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
        "            file.write(\"Video Name: \" + name + \"\\n\")\n",
        "            file.write(\"Video URL: \" + url + \"\\n\")\n",
        "            file.write(\"Video summary: \" + sum_subtitle + \"\\n\")\n",
        "            file.write(\"Video Subtitle: \" + subtitle + \"\\n\")\n",
        "        print(sum_subtitle)\n",
        "        #print(\"Video information stored in\", file_path)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "from youtube_video_search import get_video_info_from_youtube_crawler\n",
        "import streamlit as st\n",
        "st.header(\"Find your videos from Youtube\")\n",
        "# 添加搜索框和提交按钮\n",
        "search_query = st.text_input(\"Enter your search query:\")\n",
        "submit_button = st.button(\"Search\")\n",
        "\n",
        "# 当用户按下回车键时，执行搜索操作\n",
        "if search_query or submit_button:\n",
        "  # 调用函数获取视频信息\n",
        "  video_names, video_urls , video_subtitles, sum_subtitles = get_video_info_from_youtube_crawler(search_query)\n",
        "  # 确定记录的数量\n",
        "  num_records = len(video_names)\n",
        "\n",
        "  # 循环输出并打印记录\n",
        "  for i in range(num_records):\n",
        "      video_name = video_names[i]\n",
        "      video_url = video_urls[i]\n",
        "      sum_subtitle = sum_subtitles[i]\n",
        "\n",
        "      # 输出当前记录的信息\n",
        "      st.write(i+1)\n",
        "      st.write(\"Video Name:\")\n",
        "      st.write(video_name)\n",
        "\n",
        "      st.write(\"Video URL:\")\n",
        "      st.write(video_url)\n",
        "\n",
        "      st.write(\"Video Subtitle summary:\")\n",
        "      st.write(sum_subtitle)\n",
        "\n",
        "      # 打印一个空行，用于分隔每条记录\n",
        "      st.write(\"---------------------------\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NgBBMUvx2Cvc",
        "outputId": "97a2dce0-7a73-41ec-c67b-512eecedaa50"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app.py & npx localtunnel --port 8501"
      ],
      "metadata": {
        "id": "Utz4SzFBtabe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}