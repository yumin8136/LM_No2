{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN8IyZDHaltt40SWoGezetI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yumin8136/LM_No2/blob/main/Youtube_Save_To_Chroma_Search_V1_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "V1:抓取Youtube的前10条视频，向量化存储到Chroma.通过搜索功能显示出对应的视频信息.\n",
        "V1.1:拿掉subtitle信息，保留Sum_subtitle信息."
      ],
      "metadata": {
        "id": "gZzcNYhBtac8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU transformers accelerate einops langchain xformers bitsandbytes youtube-transcript-api streamlit"
      ],
      "metadata": {
        "id": "XyKdp5XAqdhb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU python-dotenv load_dotenv openai tiktoken unstructured faiss-cpu"
      ],
      "metadata": {
        "id": "BNgrO0_imJwU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oNmmqDv_qJKC",
        "outputId": "20f8f65b-9510-4834-c2da-f8f05217610b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing youtube_video_search.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile youtube_video_search.py\n",
        "#Llama 2模型初始化, GPU模型设置\n",
        "from torch import cuda,bfloat16\n",
        "import transformers\n",
        "model_id = 'meta-llama/Llama-2-7b-chat-hf'\n",
        "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
        "bub_config = transformers.BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type='nf4',\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=bfloat16\n",
        ")\n",
        "hf_auth = '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'\n",
        "model_config =transformers.AutoConfig.from_pretrained(\n",
        "    model_id,\n",
        "    use_auth_token=hf_auth\n",
        ")\n",
        "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    trust_remote_code=True,\n",
        "    config=model_config,\n",
        "    quantization_config=bub_config,\n",
        "    device_map='auto',\n",
        "    use_auth_token=hf_auth\n",
        ")\n",
        "model.eval()\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
        "    model_id,\n",
        "    use_auth_token = hf_auth\n",
        ")\n",
        "stop_list = ['\\nHuman:','\\n\"\"\"\\n']\n",
        "stop_token_ids = [tokenizer(x)['input_ids'] for x in stop_list]\n",
        "import torch\n",
        "stop_token_ids = [torch.LongTensor(x).to(device) for x in stop_token_ids]\n",
        "from transformers import StoppingCriteria, StoppingCriteriaList\n",
        "class StopOnTokens(StoppingCriteria):\n",
        "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:\n",
        "        for stop_ids in stop_token_ids:\n",
        "            if torch.eq(input_ids[0][-len(stop_ids):], stop_ids).all():\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "stopping_criteria = StoppingCriteriaList([StopOnTokens()])\n",
        "generate_text = transformers.pipeline(\n",
        "    model=model, tokenizer=tokenizer,\n",
        "    return_full_text=True,  # langchain expects the full text\n",
        "    task='text-generation',\n",
        "    # we pass model parameters here too\n",
        "    #stopping_criteria=stopping_criteria,  # without this model rambles during chat\n",
        "    temperature=0.0,  # 'randomness' of outputs, 0.0 is the min and 1.0 the max\n",
        "    max_new_tokens=128, #512,  # max number of tokens to generate in the output\n",
        "    repetition_penalty=1.1  # without this output begins repeating\n",
        ")\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "llm = HuggingFacePipeline(pipeline=generate_text)\n",
        "\n",
        "\n",
        "def llm_sum(text):\n",
        "  prompt = f\"\"\"Summarize the full text in a few sentences.\n",
        "  text: ```{text}```\n",
        "  \"\"\"\n",
        "  sum_text = llm(prompt=prompt)\n",
        "  #print(sum_text)\n",
        "  return sum_text\n",
        "#通过url获取视频字母\n",
        "def get_video_subtitle(url):\n",
        "  try:\n",
        "    from langchain.document_loaders import YoutubeLoader\n",
        "    loader = YoutubeLoader.from_youtube_url(url)\n",
        "    subtitle_list = loader.load()\n",
        "    subtitle = subtitle_list[0].page_content\n",
        "  except:\n",
        "    print(\"没有找到视频的字幕\")\n",
        "    return \"None\"\n",
        "  return subtitle\n",
        "\n",
        "#V3.1 将文件分割喂给Llama2 summarize\n",
        "def sum_subtitles(texts):\n",
        "  from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "  text_splitter = RecursiveCharacterTextSplitter(chunk_size = 5000, chunk_overlap =0)\n",
        "  chunks =text_splitter.split_text(texts)\n",
        "\n",
        "  sums =\"\"\n",
        "  sum_subtitles =[]\n",
        "  for i in range(len(chunks)):\n",
        "    sum_subtitle = llm_sum(chunks[i])\n",
        "    sum_subtitles.append(sum_subtitle)\n",
        "    #print(str(i)+\":\"+sum_subtitle+\"\\n\")\n",
        "    sums = sums + sum_subtitles[i] +\"\\n\"\n",
        "  print(sums)\n",
        "  #sum = llm(sums)\n",
        "  return sums\n",
        "\n",
        "#输入关键词，获取批量相关youtube视频信息,返回视频名,视频url,视频字幕，获取前5条记录\n",
        "def get_video_info_from_youtube_crawler(keyword):\n",
        "    import requests\n",
        "    import re\n",
        "    import json\n",
        "    url = \"https://m.youtube.com/results?search_query=\" + keyword\n",
        "    print(\"通过爬虫获取数据\")\n",
        "    resp = requests.get(url)\n",
        "\n",
        "    if resp.status_code == 200:\n",
        "        result_json = re.findall(r'ytInitialData = (.*);</script>', resp.text)[0]\n",
        "        result_obj = json.loads(result_json)\n",
        "\n",
        "        video_names = []\n",
        "        video_urls = []\n",
        "        video_subtitles = []\n",
        "        summ_subtitles = []\n",
        "\n",
        "        try:\n",
        "            contents = result_obj['contents']['twoColumnSearchResultsRenderer']['primaryContents']['sectionListRenderer']['contents']\n",
        "            count = 0\n",
        "            for content in contents:\n",
        "                if 'itemSectionRenderer' in content and 'contents' in content['itemSectionRenderer']:\n",
        "                    videos = content['itemSectionRenderer']['contents']\n",
        "                    for video in videos:\n",
        "                        if 'videoRenderer' in video:\n",
        "                            video_id = video['videoRenderer']['videoId']\n",
        "                            video_name = video['videoRenderer']['title']['runs'][0]['text']\n",
        "                            video_url = \"https://www.youtube.com/watch?v=\" + video_id\n",
        "                            video_subtitle = get_video_subtitle(video_url)\n",
        "                            video_names.append(video_name)\n",
        "                            video_urls.append(video_url)\n",
        "                            video_subtitles.append(video_subtitle)\n",
        "                            sum_subtitle = sum_subtitles(video_subtitle)\n",
        "                            summ_subtitles.append(sum_subtitle)\n",
        "                            count += 1\n",
        "                            #获取前五条记录\n",
        "                            if count >= 10:\n",
        "                              break\n",
        "\n",
        "        except KeyError:\n",
        "            return [], [] , [] ,[]\n",
        "\n",
        "        return video_names, video_urls ,video_subtitles, summ_subtitles\n",
        "    else:\n",
        "        return [], [], [], []\n",
        "\n",
        "#将视频名，视频url,视频字幕，路径等信息保存到本地\n",
        "def store_video_info_to_files(video_names, video_urls, folder_path ,sum_subtitles):\n",
        "    import re\n",
        "    for name, url, subtitle ,sum_subtitle in zip(video_names, video_urls ,video_subtitles ,sum_subtitles):\n",
        "        file_name = re.sub(r'[\\/:*?\"<>|]', '_', name) + \".txt\"\n",
        "        #file_name = name + \".txt\"\n",
        "        file_path = folder_path + \"/\" + file_name\n",
        "        #print(file_path)\n",
        "        print(name)\n",
        "        print(url)\n",
        "        with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
        "            file.write(\"Video Name: \" + name + \"\\n\")\n",
        "            file.write(\"Video URL: \" + url + \"\\n\")\n",
        "            file.write(\"Video summary: \" + sum_subtitle + \"\\n\")\n",
        "        print(sum_subtitle)\n",
        "        #print(\"Video information stored in\", file_path)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from youtube_video_search import get_video_info_from_youtube_crawler\n",
        "folder_path =\"/content\"\n",
        "search_query = \"Jung\"\n",
        "video_names, video_urls , video_subtitles, sum_subtitles = get_video_info_from_youtube_crawler(search_query)"
      ],
      "metadata": {
        "id": "NgBBMUvx2Cvc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from youtube_video_search import store_video_info_to_files\n",
        "store_video_info_to_files(video_names, video_urls, folder_path ,sum_subtitles)"
      ],
      "metadata": {
        "id": "cHo9i0ZskUid"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile save_doc_to_faiss.py\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "embeddings = OpenAIEmbeddings()\n",
        "\n",
        "def documents2dict(documents):\n",
        "    # 将Document对象列表转换为字典\n",
        "    documents_dict = [\n",
        "        {'page_content': document.page_content, 'metadata': document.metadata}\n",
        "        for document in documents\n",
        "    ]\n",
        "    return documents_dict\n",
        "\n",
        "\n",
        "def save_documents(documents,index = \"faiss_index\"):\n",
        "  from langchain.vectorstores import FAISS\n",
        "  print(\"documents:\",documents)\n",
        "  from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "  text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000,chunk_overlap=0)\n",
        "  docs = text_splitter.split_documents(documents)\n",
        "  print(\"docs:\",docs)\n",
        "  db = FAISS.from_documents(docs, embeddings)\n",
        "  db.save_local(index)\n",
        "  return db\n",
        "\n",
        "def get_documents(index=\"faiss_index\", query=\"\", limit=3):\n",
        "    from langchain.vectorstores import FAISS\n",
        "    db = FAISS.load_local(index, embeddings)\n",
        "    docs = db.similarity_search(query, k=limit)\n",
        "    txts = documents2dict(docs)\n",
        "    print(\"txts:\",txts)\n",
        "    return txts\n",
        "\n",
        "from langchain import document_loaders\n",
        "from langchain.document_loaders import DirectoryLoader\n",
        "dir = '/content'\n",
        "loader =DirectoryLoader(dir,glob='**/*.txt')\n",
        "documents = loader.load()\n",
        "\n",
        "##加载数据到faiss\n",
        "save_documents(documents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uu8Zh_sxitnr",
        "outputId": "ff6b33c4-04a9-4a1a-c27f-c0e1024d9c7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting save_doc_to_faiss.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "from save_doc_to_faiss import get_documents\n",
        "import streamlit as st\n",
        "index=\"faiss_index\"\n",
        "st.header(\"search your file\")\n",
        "search_query = st.text_input(\"Enter your search query:\")\n",
        "#search_query = \"what is funetuning?\"\n",
        "submit_button = st.button(\"Search\")\n",
        "\n",
        "if search_query or submit_button:\n",
        "  txts = get_documents(index,search_query,3)\n",
        "  for index, doc in enumerate(txts):\n",
        "    page_content = doc['page_content']\n",
        "    source = doc['metadata']['source']\n",
        "    st.write(f'Document {index+1}:')\n",
        "    st.write('Page Content:' + page_content)\n",
        "    st.write('Source:'+ source)\n",
        "    print(f'Document {index+1}:')\n",
        "    print('Page Content:' + page_content)\n",
        "    print('Source:'+ source)\n",
        "    print(\"-----------------------\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "reYeoGPji49E",
        "outputId": "91d55285-5d1b-48f2-dea0-334948dc118a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app.py & npx localtunnel --port 8501"
      ],
      "metadata": {
        "id": "Utz4SzFBtabe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}